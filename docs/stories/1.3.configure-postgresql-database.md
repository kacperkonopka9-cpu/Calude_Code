# Story 1.3: Configure PostgreSQL database with initial schema

## Status
**Ready for Review**

## Story

**As a** Developer,
**I want** a PostgreSQL database configured with SQLAlchemy ORM and the initial schema for users, batches, and projects,
**so that** I can persist application data with type-safe models and implement the repository pattern for data access.

## Acceptance Criteria

1. PostgreSQL 15+ database accessible from FastAPI application via asyncpg driver
2. SQLAlchemy 2.0+ configured with async engine and sessions for database access
3. Alembic configured for database migrations with initial migration script
4. Database models created for core tables: users, batches, projects, project_contents, generation_metadata
5. Repository pattern implemented for at least one model (User or Batch) demonstrating CRUD operations
6. Database connection configuration loaded from environment variables via Settings class
7. Database session dependency created for FastAPI route injection
8. At least 3 unit tests demonstrating repository pattern with test database
9. Database initialization script that creates all tables and extensions (uuid-ossp)
10. Health check endpoint updated to verify database connectivity
11. All database access uses async SQLAlchemy (no raw SQL queries)
12. Type hints present on all repository methods and model relationships

## Tasks / Subtasks

- [ ] **Task 1: Install PostgreSQL dependencies** (AC: 1, 2)
  - [ ] Add to `apps/api/requirements.txt`: SQLAlchemy>=2.0, asyncpg, alembic, psycopg2-binary (for Alembic)
  - [ ] Install dependencies: `py -m pip install -r requirements.txt`
  - [ ] Verify asyncpg driver is available

- [ ] **Task 2: Configure database connection** (AC: 1, 6)
  - [ ] Update `apps/api/src/config.py` to ensure DATABASE_URL is present (already defined)
  - [ ] Create `apps/api/src/database.py` with async engine and session factory
  - [ ] Implement `get_async_session()` generator function for FastAPI Depends injection
  - [ ] Add connection pooling configuration (pool_size=20, max_overflow=10)
  - [ ] Document database configuration in `apps/api/README.md`

- [ ] **Task 3: Initialize Alembic for migrations** (AC: 3)
  - [ ] Run `alembic init alembic` from `apps/api/` directory
  - [ ] Update `alembic.ini` to use async SQLAlchemy URL from config
  - [ ] Update `alembic/env.py` to import models and use async engine
  - [ ] Configure Alembic to detect model changes automatically
  - [ ] Test Alembic configuration: `alembic check`

- [ ] **Task 4: Create SQLAlchemy models for core tables** (AC: 4, 12)
  - [ ] Create `apps/api/src/models/base.py` with Base declarative base and common fields
  - [ ] Create `apps/api/src/models/user.py` with User model (id, email, name, company, cognito_sub, preferences, timestamps)
  - [ ] Create `apps/api/src/models/batch.py` with Batch model (id, user_id, name, status, total_projects, completed_projects, average_quality_score, timestamps, expires_at)
  - [ ] Create `apps/api/src/models/project.py` with Project model (id, batch_id, row_number, status, quality_score, input_data, ai_model, timestamps)
  - [ ] Create `apps/api/src/models/project_content.py` with ProjectContent model (project_id, sections, version, last_edited_section)
  - [ ] Create `apps/api/src/models/generation_metadata.py` with GenerationMetadata model (project_id, attempts, tokens, cost, latency, model_used, errors, completed_at)
  - [ ] Add type hints to all model fields and relationships
  - [ ] Update `apps/api/src/models/__init__.py` to export all models

- [ ] **Task 5: Create initial Alembic migration** (AC: 3, 9)
  - [ ] Generate initial migration: `alembic revision --autogenerate -m "Initial schema: users, batches, projects"`
  - [ ] Review generated migration file and add UUID extension creation: `op.execute('CREATE EXTENSION IF NOT EXISTS "uuid-ossp"')`
  - [ ] Ensure all constraints, indexes, and triggers from architecture/9-database-schema.md are present
  - [ ] Add database trigger for updating batch quality score (if not auto-generated)
  - [ ] Test migration: `alembic upgrade head` (on local PostgreSQL instance)

- [ ] **Task 6: Implement repository pattern** (AC: 5, 11, 12)
  - [ ] Create `apps/api/src/repositories/base_repository.py` with generic repository base class
  - [ ] Create `apps/api/src/repositories/user_repository.py` with UserRepository:
    - `async def create(email, name, cognito_sub) -> User`
    - `async def get_by_id(user_id: UUID) -> Optional[User]`
    - `async def get_by_email(email: str) -> Optional[User]`
    - `async def update_last_login(user_id: UUID) -> None`
  - [ ] Create `apps/api/src/repositories/batch_repository.py` with BatchRepository:
    - `async def create(user_id, name, total_projects) -> Batch`
    - `async def get_by_id(batch_id: UUID) -> Optional[Batch]`
    - `async def list_by_user(user_id: UUID, limit: int, offset: int) -> List[Batch]`
    - `async def update_status(batch_id: UUID, status: str) -> None`
  - [ ] Ensure all repository methods use AsyncSession and return type hints
  - [ ] Update `apps/api/src/repositories/__init__.py` to export repositories

- [ ] **Task 7: Create database session dependency for FastAPI** (AC: 7)
  - [ ] Implement `get_db()` async generator in `apps/api/src/database.py` for Depends injection
  - [ ] Create example route in `apps/api/src/routes/users.py` demonstrating session injection
  - [ ] Document dependency injection pattern in `apps/api/README.md`

- [ ] **Task 8: Write unit tests for repositories** (AC: 8)
  - [ ] Update `apps/api/tests/conftest.py` to add test database fixtures:
    - `async_engine` fixture for test database engine
    - `async_session` fixture for test database session
    - `setup_test_db` fixture to create/drop tables for each test
  - [ ] Create `apps/api/tests/test_database.py` to test database connection and session creation
  - [ ] Create `apps/api/tests/test_user_repository.py` with tests:
    - `test_create_user_success`
    - `test_get_user_by_id_found`
    - `test_get_user_by_id_not_found`
    - `test_get_user_by_email`
  - [ ] Create `apps/api/tests/test_batch_repository.py` with tests:
    - `test_create_batch_success`
    - `test_list_batches_by_user`
    - `test_update_batch_status`
  - [ ] Ensure all tests use test database (not production database)
  - [ ] Run tests: `pytest tests/ -v` and verify all pass

- [ ] **Task 9: Update health check endpoint** (AC: 10)
  - [ ] Update `/health` endpoint in `apps/api/src/main.py` to verify database connectivity
  - [ ] Add database ping query: `await session.execute(text("SELECT 1"))`
  - [ ] Return database status in health check response: `{"status": "healthy", "version": "1.0.0", "database": "connected"}`
  - [ ] Write test for updated health check in `apps/api/tests/test_main.py`

- [ ] **Task 10: Create database initialization script** (AC: 9)
  - [ ] Create `apps/api/scripts/init_db.py` script to create all tables and extensions
  - [ ] Script should use Alembic `upgrade head` command
  - [ ] Add script to README.md under "Database Setup" section
  - [ ] Test script on clean database

- [ ] **Task 11: Update configuration and documentation** (AC: 6)
  - [ ] Verify DATABASE_URL in `.env.example` is correct: `postgresql+asyncpg://postgres:postgres@localhost:5432/rnd_cards`
  - [ ] Add database setup instructions to `apps/api/README.md`:
    - How to install PostgreSQL locally
    - How to create database: `CREATE DATABASE rnd_cards;`
    - How to run migrations: `alembic upgrade head`
  - [ ] Add database environment variables table to README
  - [ ] Document repository pattern and usage examples

- [ ] **Task 12: Verify all acceptance criteria** (AC: 1-12)
  - [ ] Run all tests: `pytest tests/ --cov=src`
  - [ ] Verify 100% test coverage for repositories
  - [ ] Run type checking: `mypy src/`
  - [ ] Run linting: `ruff check src/`
  - [ ] Start server and verify `/health` returns database connection status
  - [ ] Verify Alembic migrations work: `alembic downgrade -1` then `alembic upgrade head`

## Dev Notes

### Story Context

This is **Story 1.3** in Epic 1 (Infrastructure Sprint 0). You are continuing the backend foundation started in Story 1.2 (FastAPI setup). The goal is to add PostgreSQL database connectivity with SQLAlchemy ORM, implementing the repository pattern for clean data access.

**Previous Story (1.2) Achievements:**
- FastAPI application with CORS, health check, and OpenAPI docs
- Layered architecture: routes/, services/, repositories/, models/, schemas/, workers/, middleware/, utils/
- Environment configuration with Pydantic Settings
- pytest-asyncio testing framework with httpx.AsyncClient
- 100% test coverage, zero linting/type errors
- Ruff linting, mypy strict mode type checking

**This Story (1.3) Focus:**
- Add PostgreSQL database with async SQLAlchemy 2.0
- Implement Alembic migrations for schema management
- Create SQLAlchemy models matching architecture/9-database-schema.md
- Implement repository pattern for data access (no raw SQL)
- Add database session dependency injection for FastAPI routes
- Maintain 100% test coverage with test database fixtures

**Next Story (1.4) Preview:**
- Story 1.4 will add S3 file storage for Excel uploads and Word exports
- Database will be needed to store batch metadata and S3 file references

### Architecture Context

**Source:** `docs/architecture/9-database-schema.md`

#### Core Database Tables

1. **users** - Polish tax consultants using the system
   - Fields: id (UUID), email, name, company, cognito_sub, preferences (JSONB), timestamps
   - Indexes: email, cognito_sub
   - Relationship: 1:N with batches

2. **batches** - Groups of 1-20 project cards processed together
   - Fields: id, user_id, name, status, total_projects, completed_projects, average_quality_score, timestamps, expires_at (created_at + 90 days)
   - Status constraint: uploading, validating, validation_failed, queued, processing, completed, partially_completed, archived
   - Indexes: user_id, status, created_at DESC, expires_at
   - Relationship: N:1 with users, 1:N with projects

3. **projects** - Individual R&D project cards
   - Fields: id, batch_id, row_number, status, quality_score, input_data (JSONB), ai_model, timestamps
   - Status constraint: pending, generating, completed, failed, reviewed, exported
   - Indexes: batch_id, status
   - Unique constraint: (batch_id, row_number)
   - Relationship: N:1 with batches, 1:1 with project_contents, 1:1 with generation_metadata

4. **project_contents** - Generated content for 8 Ulga B+R sections (1:1 with projects)
   - Fields: project_id (PK), sections (JSONB), version, last_edited_section
   - Structure: `sections` is JSONB with keys: nazwa_projektu, cel_projektu, opis_projektu, nowatorstwo, metodologia, rezultaty, koszty, harmonogram

5. **generation_metadata** - AI usage tracking for cost monitoring (1:1 with projects)
   - Fields: project_id (PK), attempts, total_input_tokens, total_output_tokens, total_cost_eur, average_latency_ms, model_used, errors (JSONB), completed_at
   - Used for Epic 11 (Performance Optimization & Monitoring)

**Database Trigger:**
- `update_batch_quality_score()` trigger fires on projects.quality_score or projects.status UPDATE
- Automatically recalculates batch average_quality_score and completed_projects count
- See architecture/9-database-schema.md lines 116-138 for SQL implementation

#### Tech Stack Requirements

**Source:** `docs/architecture/3-tech-stack.md`

- **Database:** PostgreSQL 15+ (JSONB for flexible content, ACID guarantees)
- **Backend Language:** Python 3.11+
- **Backend Framework:** FastAPI 0.104+ (async support, Pydantic validation)
- **Backend Testing:** pytest 7.4+ with pytest-asyncio (async FastAPI support)
- **Database Driver:** asyncpg (async PostgreSQL driver for SQLAlchemy)

**Dependencies to add:**
```
SQLAlchemy>=2.0.0
asyncpg>=0.29.0
alembic>=1.12.0
psycopg2-binary>=2.9.9  # Required by Alembic for metadata operations
```

#### Repository Pattern Requirements

**Source:** `docs/architecture/11-backend-architecture.md` lines 115-169

**Example Repository Pattern:**
```python
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update
from typing import List, Optional
from uuid import UUID

from ..models.batch import Batch

class BatchRepository:
    def __init__(self, session: AsyncSession):
        self.session = session

    async def create(self, user_id: UUID, name: str, total_projects: int) -> Batch:
        batch = Batch(
            user_id=user_id,
            name=name,
            total_projects=total_projects
        )
        self.session.add(batch)
        await self.session.commit()
        await self.session.refresh(batch)
        return batch

    async def get_by_id(self, batch_id: UUID) -> Optional[Batch]:
        result = await self.session.execute(
            select(Batch).where(Batch.id == batch_id)
        )
        return result.scalar_one_or_none()

    async def list_by_user(
        self,
        user_id: UUID,
        limit: int = 10,
        offset: int = 0
    ) -> List[Batch]:
        result = await self.session.execute(
            select(Batch)
            .where(Batch.user_id == user_id, Batch.status != 'archived')
            .order_by(Batch.created_at.desc())
            .limit(limit)
            .offset(offset)
        )
        return result.scalars().all()

    async def update_status(self, batch_id: UUID, status: str) -> None:
        await self.session.execute(
            update(Batch)
            .where(Batch.id == batch_id)
            .values(status=status)
        )
        await self.session.commit()
```

**Key Patterns:**
- Repository class accepts `AsyncSession` in constructor
- All database operations use `async/await`
- Use SQLAlchemy `select()`, `update()`, `insert()` - **NEVER raw SQL**
- Return type hints on all methods (Optional[Model], List[Model], etc.)
- Commit and refresh after inserts to get generated IDs

### Coding Standards

**Source:** `docs/architecture/17-coding-standards.md`

**Critical Fullstack Rules:**
- **Database Access:** Never write raw SQL - use SQLAlchemy ORM and repository pattern
- **Environment Variables:** Access only through config objects, never `os.environ` directly
- **State Updates:** Never mutate state directly - use proper state management patterns

**Naming Conventions:**
- Database Tables: snake_case (`user_profiles`)
- Functions (Python): snake_case (`validate_excel()`)
- Constants: UPPER_SNAKE_CASE (`MAX_BATCH_SIZE`)
- Interfaces/Types: PascalCase (`UserProfile`)

**Code Quality Gates:**
- Ruff/mypy passes (backend)
- Unit tests pass
- Type checking passes
- Code coverage ≥80% for new code

### Testing Strategy

**Source:** `docs/architecture/16-testing-strategy.md`

**Backend Test Organization:**
```
apps/api/tests/
├── unit/
│   ├── services/
│   ├── repositories/  ← Focus for Story 1.3
│   └── models/
├── integration/
│   ├── test_batches_api.py
│   └── test_projects_api.py
└── e2e/
```

**Backend Test Example (from architecture):**
```python
import pytest
from httpx import AsyncClient
from sqlalchemy.ext.asyncio import AsyncSession

@pytest.mark.asyncio
async def test_create_batch_with_valid_data(client: AsyncClient, auth_token: str):
    # Test implementation
    pass
```

**Test Requirements for Story 1.3:**
- Use pytest fixtures for test database setup/teardown
- Create separate test database (not production)
- Test all repository CRUD operations
- Verify database connection in health check test
- Aim for 100% coverage on repository code

### Database Configuration Pattern

**Source:** `apps/api/src/config.py` (from Story 1.2)

```python
from functools import lru_cache
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # Database (Story 1.3)
    DATABASE_URL: str = "postgresql+asyncpg://postgres:postgres@localhost:5432/rnd_cards"

    class Config:
        env_file = ".env"
        case_sensitive = True

@lru_cache
def get_settings() -> Settings:
    return Settings()

settings = get_settings()
```

**Database Connection Setup (to create in Task 2):**
```python
# apps/api/src/database.py
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from .config import settings

engine = create_async_engine(
    settings.DATABASE_URL,
    echo=False,  # Set to True for SQL query logging in development
    pool_size=20,
    max_overflow=10
)

AsyncSessionLocal = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)

async def get_db() -> AsyncSession:
    async with AsyncSessionLocal() as session:
        yield session
```

### Alembic Configuration

**Source:** BMad best practices for database migrations

**Initial Setup:**
1. Run `alembic init alembic` from `apps/api/` directory
2. Update `alembic.ini` to use async SQLAlchemy URL
3. Update `alembic/env.py` to import models and configure async engine

**Example `alembic/env.py` (async configuration):**
```python
from logging.config import fileConfig
from sqlalchemy import pool
from sqlalchemy.ext.asyncio import async_engine_from_config
from alembic import context

# Import your models here
from src.models import Base  # Imports all models via __init__.py
from src.config import settings

# Alembic Config object
config = context.config

# Override sqlalchemy.url with DATABASE_URL from settings
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

# Model metadata
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    # Offline migration mode (generates SQL without executing)
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )
    with context.begin_transaction():
        context.run_migrations()

async def run_migrations_online() -> None:
    # Online migration mode (connects to database)
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()

def do_run_migrations(connection):
    context.configure(connection=connection, target_metadata=target_metadata)
    with context.begin_transaction():
        context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    import asyncio
    asyncio.run(run_migrations_online())
```

### SQLAlchemy Model Pattern

**Base Model (to create in Task 4):**
```python
# apps/api/src/models/base.py
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy import func
from datetime import datetime
from uuid import UUID, uuid4

class Base(DeclarativeBase):
    pass

class TimestampMixin:
    created_at: Mapped[datetime] = mapped_column(
        server_default=func.now(),
        nullable=False
    )
```

**Example User Model:**
```python
# apps/api/src/models/user.py
from sqlalchemy import String, TIMESTAMP
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.orm import Mapped, mapped_column
from uuid import uuid4
from datetime import datetime
from typing import Optional

from .base import Base, TimestampMixin

class User(Base, TimestampMixin):
    __tablename__ = "users"

    id: Mapped[UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid4)
    email: Mapped[str] = mapped_column(String(255), unique=True, nullable=False, index=True)
    name: Mapped[str] = mapped_column(String(255), nullable=False)
    company: Mapped[Optional[str]] = mapped_column(String(255), nullable=True)
    cognito_sub: Mapped[str] = mapped_column(String(255), unique=True, nullable=False, index=True)
    preferences: Mapped[dict] = mapped_column(
        JSONB,
        nullable=False,
        default={"defaultAiModel": "claude", "language": "pl", "notificationsEnabled": True}
    )
    last_login_at: Mapped[Optional[datetime]] = mapped_column(TIMESTAMP(timezone=True), nullable=True)
```

### Test Database Fixtures

**Source:** pytest-asyncio best practices

**Example `conftest.py` additions:**
```python
# apps/api/tests/conftest.py
import pytest
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

from src.models.base import Base
from src.database import get_db
from src.main import app

# Test database URL (use separate test database)
TEST_DATABASE_URL = "postgresql+asyncpg://postgres:postgres@localhost:5432/rnd_cards_test"

@pytest.fixture
async def async_engine():
    engine = create_async_engine(TEST_DATABASE_URL, echo=False)
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    yield engine
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)
    await engine.dispose()

@pytest.fixture
async def async_session(async_engine):
    AsyncSessionLocal = sessionmaker(
        async_engine,
        class_=AsyncSession,
        expire_on_commit=False
    )
    async with AsyncSessionLocal() as session:
        yield session

@pytest.fixture
def override_get_db(async_session):
    async def _override_get_db():
        yield async_session
    app.dependency_overrides[get_db] = _override_get_db
    yield
    app.dependency_overrides.clear()
```

### Local PostgreSQL Setup Instructions

**For Windows (user's environment):**

1. **Install PostgreSQL:**
   ```bash
   # Using winget (Windows Package Manager)
   winget install PostgreSQL.PostgreSQL

   # Or download from: https://www.postgresql.org/download/windows/
   ```

2. **Create Database:**
   ```bash
   # Connect to PostgreSQL (default user: postgres)
   psql -U postgres

   # Create development database
   CREATE DATABASE rnd_cards;

   # Create test database
   CREATE DATABASE rnd_cards_test;

   # Exit psql
   \q
   ```

3. **Update .env File:**
   ```bash
   DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/rnd_cards
   ```

4. **Run Migrations:**
   ```bash
   cd apps/api
   alembic upgrade head
   ```

**Note for Developer:** If PostgreSQL is not installed, the story implementation should document these setup steps in README.md and proceed with creating all code. Tests will require PostgreSQL to be running locally.

### Success Criteria Verification

**After completing all tasks, verify:**

1. ✅ PostgreSQL 15+ database accessible via asyncpg
2. ✅ SQLAlchemy 2.0+ configured with async engine and sessions
3. ✅ Alembic configured with initial migration
4. ✅ All 5 core models created (users, batches, projects, project_contents, generation_metadata)
5. ✅ Repository pattern implemented for User and Batch
6. ✅ Database configuration loaded from Settings class
7. ✅ Database session dependency `get_db()` created
8. ✅ At least 6 unit tests (3+ for UserRepository, 3+ for BatchRepository)
9. ✅ Database initialization script created
10. ✅ Health check endpoint returns database connection status
11. ✅ All database access uses async SQLAlchemy (zero raw SQL)
12. ✅ Type hints on all repository methods and model fields

**Quality Gates:**
- `pytest tests/ --cov=src` - All tests pass, 100% coverage on repositories
- `mypy src/` - Zero type errors
- `ruff check src/` - Zero linting errors
- Server starts and `/health` returns `{"status": "healthy", "version": "1.0.0", "database": "connected"}`

### Learning from Story 1.2

**Apply these patterns from Story 1.2:**
- ✅ Comprehensive docstrings on all modules and functions
- ✅ Type hints on all function signatures
- ✅ Separate pytest.ini to avoid build conflicts
- ✅ .env.example with clear variable documentation
- ✅ README.md with setup instructions and examples
- ✅ 100% test coverage as baseline (≥80% minimum)

**Story 1.2 Dev Agent Record - Database Insights:**

From Story 1.2 QA review, the following recommendations are relevant for Story 1.3:
- Story 1.2 had a cosmetic Pydantic deprecation warning (class-based Config). For Story 1.3, use `model_config = ConfigDict(...)` pattern if creating new Pydantic models (Schemas).
- Story 1.2 referenced Story 1.3 database setup in README: "When adding database: Use async SQLAlchemy with asyncpg driver"
- Story 1.2 structured `apps/api/` with placeholder directories. Story 1.3 will populate `models/` and `repositories/` directories.

### Troubleshooting Notes

**If PostgreSQL installation fails:**
- Document all code as production-ready (models, repositories, migrations)
- Include comprehensive setup instructions in README.md
- Tests will require PostgreSQL to run, but code quality can be verified with mypy/ruff
- Use Story 1.2 pattern: complete implementation, then verify when environment is ready

**Python Command:**
- Use `py` command (Python launcher for Windows) as established in Story 1.2
- Commands: `py -m pip install`, `py -m pytest`, etc.

---

## QA Coordination

**Pre-Development QA (Recommended):**
- [ ] *Risk Profile* (`*risk docs/stories/1.3.configure-postgresql-database.md`) - Identify integration/regression risks before starting
- [ ] *Test Design* (`*design docs/stories/1.3.configure-postgresql-database.md`) - Create test strategy to guide development

**Post-Development QA (Required):**
- [ ] *Story Review* (`*review docs/stories/1.3.configure-postgresql-database.md`) - Comprehensive quality assessment after implementation
- [ ] *QA Gate* (`*gate docs/stories/1.3.configure-postgresql-database.md`) - Final PASS/CONCERNS/FAIL decision

---

## Definition of Done

- [ ] All 12 acceptance criteria met
- [ ] All 12 tasks completed and verified
- [ ] All unit tests passing (pytest)
- [ ] Code coverage ≥80% (target: 100% for repositories)
- [ ] No mypy type errors (strict mode)
- [ ] No ruff linting errors
- [ ] Database migrations tested (upgrade/downgrade)
- [ ] Health check endpoint returns database status
- [ ] README.md updated with database setup instructions
- [ ] QA review completed with PASS gate
- [ ] Story status updated to "Ready for Done"

---

## Notes

**Epic Reference:** Epic 1 - Infrastructure Sprint 0 (Story 3 of 7)

**Related Stories:**
- Story 1.1: Set up React frontend ✅ Done (QA Score: 95/100)
- Story 1.2: Set up FastAPI backend ✅ Done (QA Score: 98/100)
- **Story 1.3: Configure PostgreSQL database** ← Current Story
- Story 1.4: Set up S3 file storage (depends on Story 1.3 for batch metadata)
- Story 1.5: Implement CI/CD pipeline
- Story 1.6: Create infrastructure as code
- Story 1.7: Set up local development environment documentation

**Architecture Documents Referenced:**
- `docs/architecture/3-tech-stack.md` - Technology selection (PostgreSQL 15+, SQLAlchemy 2.0+, asyncpg)
- `docs/architecture/9-database-schema.md` - Complete DDL schema with constraints, indexes, triggers
- `docs/architecture/11-backend-architecture.md` - Repository pattern example, async SQLAlchemy usage
- `docs/architecture/17-coding-standards.md` - Naming conventions, quality gates
- `docs/architecture/16-testing-strategy.md` - Test organization, pytest patterns

**Developer:** James (BMad Developer Agent)
**Created:** 2025-10-26
